Ok, this has nothing to do with JavaScript, but its easier for me to just bunch my notes here in with the Javascript notes.

Doing a quick Course on Splunk

So what is Splunk?

Think of data, all data produced by humans and machines, from Structured Data, Unstructured data, and Machine code, etc. Splunk takes that data and makes it useful by producing reports, alerts, and dashboards.

Its like modern organization.

So Plunk takes all this data (like how a machine logs everything), logs it, cleans it, formats it, and makes it sing.

Splunk is especially great at reading machine data. Its biggest superpower is parsing Machine data. (Data generated by machines (log data thats usally encrypted and not human readable))

Splunk can look for patterns in that unreadable data, and label that data with feilds/names.

We can take those feilds and build intellegence systems that show us the data so we can understand it.

Another way to put it, Splunk turns Unstructured data into insights.

some rules with Splunk

1. The best place to get help with Splunk is the Splunk Community Answers
2. Splunk IS NOT a bussiness intelligence tool. It is an IT operations tool that consumes machine-data in real time. Most Bussiness intelligence tools do not consume and analyze data in real time.
3. Splunk can be used to investigate, alert, build dashboards, and build reports.

Now - Deploying Spunk

but first a note on configuraton files

1. everything Splunk does is governed by configuration files.
2. Configuration files are stored in the /etc, and they have the .conf extension
3. Confugration files are layered. You can have .conf files that have the same name in different directories.
4. NEVER EDIT .conf FILES IN THE DEFAULT DIRECTORY
5. The /etc/<app>/default directory contains preconfigured versions of .conf files.
6. Instead use the Local Directory
7. The /etc/<app>/local directory is where the custom configurations are stored.

Now the Configuration File Structure
.conf files are build on Stanza's and then underneath it attribute values

[Stanza]
Attribute = Value

Splunk offers 2 main deployment models.
Cloud & On Premises

regarldess which deployment you use the data pipeline is the same
IPIS
input - forwarded data, uploaded data, network data, scripts (Splunk is consuming data, but doens't look at it)
parsing - examines the data, addes metadata (Splunk looks at the data)
indexing - data divided into events. Wrties the data to the disk in "buckets" (stores its work in directories, it calls buckets)
searching - user interaction with the data. (self explanetory)

To scale your system, you can spread the functionality accross multiple specialized instances of splunk. No matter how large you scale, you installing only one of two types of splunk instances, a universal fowarder, or a splunk enterprise instalation

This all makes up every splunk deployments

Forwarder =>
(then below the Deployer)
Heavy Forwarder => Search Head => Deployment Server
Master Cluster Node => License Server => Indexer

The 1st deployment, liek personal deployments -

So when you install Splunk on a Computer, that instalation serves as a Search Head and an Indexer that has up to 10 fowarders and appropriate for up to 10 users

The 2nd deployment is enterpise deployment

You have a single search head, two to three indexers, 100-20 fowarders, and appropriate for 100 - 200 users

A 3rd type of Deployment is a Distributed Search

In this config, you build search head clusters governed by a deployer. It distributed files and configuration updates to all members through a configuration bundle.

A search head cluster is a group of search heads that serve as a central resource for searching. So users can log into any member of the search head cluster and take advantage of clustering a bunch of search heads together. (for performance)

Users can run the same searchs and view the same data for any search head in the cluster

One head is designated as the Search Head Captian and schedals jobs between the clusters

With this deployment there are many indexers and thousands of load-balanced forwarders

The 4th type of deployment is has near infinite scalability. In this config,
called a "Large Enterprise Deployment"

It can have Search Head Clusters, or Indexer Clusters, or Both
And it can have thousands of load-balanced fowarders

- Now how does Splunk Store Data

Splunk adds data to Indexes, the indexes match the places on the disk that Splunk calls buckets.

Splunk comes with several indexes built in, and you can create you own as well
Splunk transforms incoming data into events, and stores it in indexes.
An event is a single row of data that has metadata attached to it.
The default index of Splunk is main and it also has an \_internal index that stores internal logs and a few other indexes that come with splunk.

Again, an event is a single row of data
Data is specified by fields (key values pairs) (like objects in JavaScript)
Splunk adds default fields to all events - Timestamp (time), host (ip address), source (the file from where the source originates), sourcetype (the format of the data)

In Splunk, an index contains compressed raw data from an associated index files, and they are spread out into different directory files depending on their age.

There are 6 default buckets, but only 5 important for us to know about

The 6th bucket is the Fish Bucket and its more advanced

1. Hot Bucket (hotpath) is for newly indexed data for it be Read/Write
2. Warm (warmPath) Contains data rolled from Hot Buckets with no active writing (So Read only) And index has many Warm buckets
3. Cold (coldPath) Takes data from warm buckets and moves it to a different locaton. An index has many cold buckets
4. Frozen (frozenPath) data rolled from cold, where data is deleted or archived
5. Thawed (thawedPath) data resored from an Archive

6. Fish Bucket (this is more advanced so we will not talk about it yet)

With Buckets, you can set up retirerment, archive, back up, configure index sizes, partition index data

Splunk Licensing

You license data ingested per day, not data stored

Daily indexing volume is measured from midnight to midnight by the clock on the license master

Internal Log Data, Duplicate Data, and Meta Data do not count against your licesne.

There are 5 splunk licesne types
Standard / Enterprise/ Sales Trial / Dev Test / Free(barebones with ample restrictions)

There is also a few speciality liscense for Industral Lot and Forwarder

If you exceed your licesne limit within a day, you get a warning
get 5 warnings in a 30 day period, you are offically in violation
If you're in violation, it will annoy you with notices, but it won't be enfroced, so you don't lose functionalty. Splunk allows this to accomidate unexpected bursts of data volume

Regarding Splunk Licensing

1. license pools are created from license stacks
2. Pools are sized for specific purposes
3. Managed by the license master
4. Indexers and other Splink enterprise instances are assigned to a pool

Since fowarders have a unique license, the shouldn't be fowarded to a licesne pool

Largest container is a License Group, within those groups you can create a license stack.
A license stack is a stack of licenses added together
License pools then consume all or most of the license stack, and splunk instances are assigned licesnes out of pools.

Splunk also has Apps and Addons for those apps.

The Apps offer better Visulizaton, Analysis, Reports, Dashboards, and user interfaces

While the Add-ons can give data enrichment, tags, data models and data sets.

A App is simply a collection of .conf files
An Add on is a subset of an app, Add-ons specify do not have Graphical user interfaces cause they are add ons to other apps.

Apps are downloaded from Splunkbase.com - 1st and 3rd party apps, premium and free offerings.

Some of the free apps however will require a license.

So with learning Splunk

Getting the data in

So to reiterate, the Splunk Data Pipeline is IPIS
Inputs - Parsing - Indexing - Search

General Input Categories

- File and Directory Inputs
  - This allows you to monitor files and directories
    - Both locally and remotely
    - And you can monitor compressed files
  - Upload
    - upload files to splunk or to a remote Heavy Fowarder
    - used for a one-time analysis
  - MonitorNoHandle
    - Available for Windows Hosts only
    - Monitor files and directories that the system rotates automatically.
- Network Inputs
  - Data from TCP and UDP (Splunk recomends using TCP whenever possiable)
    - syslog
  - Data from SNMP events
- Windows Inputs
  - Windows Event Logs
  - Registry
  - Active Directory
  - WMI (using fowarders is recommended over using WM)
  - Performance Monitoring
    (perfmon)

Splunk can also take in other data sources - Metrics, Scripted Inputs, Modular Inputs, and HTTP Event Collectors

- Ways to Configure Input in Splunk

  - Through an App
    - Many Apps have preconfigured inputs
  - Splunk Web
    - Settings > data inputs
    - Settings > add Data
  - CLI (Command line interface)
    - ./splunk add monitor <path>
  - Through inputs.conf
    - Add a stanza for each input
  - Guided Data Onboarding (GDO)
    - Data input Wizard

- Splunk has two main Fowarder types
  - Universal - Takes Data from a data source and fowards it to a reciever - thats all it does. The data can even be another reciever - However these aren't part of Splunk itself and required a seperate instalation
    The Universal Fowarder also does not search or parse any data, it just captures the data and fowards it on to something else
  - Heavy - An advanced fowarder part of Splunk, Unlike Universal Fowarders, a Heavy Fowarder can not foward data, but also Parse data and route data, among many other things you can't do with a universal fowarder.

Universal Fowarder Configuration Steps 1. Configure recieving on a Splunk Enterprise instance 2. Download and install the UF 3. Start the UF 4. Configure the UF to send data 5. Configure the UF to collect data from the host system

- Side Note - Installing Splunk on my Mac was kind of weird...need to try installing Splunk on Windows

- Summary
  - Splunk Data Pipeline
  - Configureing Inputs
  - Types of forwarders
  - configuring heavy fowarders
  - configuring universal fowarders

Splunk Quiz 2 -

1. The Splunk Enterprise Trail license is valid for? - 60 days
2. To collect and parse data at a source, you need a? - Heavy Fowarder
3. Splunk can be set up in a distributed environment - True
4. Splunk can be installed in the following enviromens - All of these (unix, linux, windows, mac, solaris)
5. Select the best description of a SplunK App - A collection of configuration files that extend the functionality of Splunk
6. A licensing violation is in effect for? - 30 Days
7. Splunk will stop indexing your data during the violation period. - False
8. To manage licensing in the Splunk GUI, navigate to - Settings/Licensing
9. Search is a Splunk App? - True
10. Where should you go to find and download Splunk apps? - splunkbase.com

Splunk Quiz 3 -

1. What is the default forwarding port? - 9997
2. Universal Forwarders do not parse data - True
3. Which port is the default management/deployment port - 8089
4. Which type of forwarder requires a specific type of license? - Heavy
5. On which platform can you use WGET to install a universal forwarder? All of these!
   - Linux, Unix, Windows, Solaris, A/X,
6. Other then the installation wizard, how can you configure a universal forwarder? - By editing the configuration files
7. Universal Fowarders should also be installed on all indexes? - False
8. Some syslog devices do not require Splunk Fowarders. Syslog data is generally recieved on port? - 514
9. Which of the following is not a Splunk default metadata assignment? - Network
10. Splunk can monitor both individual files and entire directories? - True
