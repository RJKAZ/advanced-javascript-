Ok, this has nothing to do with JavaScript, but its easier for me to just bunch my notes here in with the Javascript notes.

Doing a quick Course on Splunk

So what is Splunk?

Think of data, all data produced by humans and machines, from Structured Data, Unstructured data, and Machine code, etc. Splunk takes that data and makes it useful by producing reports, alerts, and dashboards.

Its like modern organization.

So Plunk takes all this data (like how a machine logs everything), logs it, cleans it, formats it, and makes it sing.

Splunk is especially great at reading machine data. Its biggest superpower is parsing Machine data. (Data generated by machines (log data thats usally encrypted and not human readable))

Splunk can look for patterns in that unreadable data, and label that data with feilds/names.

We can take those feilds and build intellegence systems that show us the data so we can understand it.

Another way to put it, Splunk turns Unstructured data into insights.

some rules with Splunk

1. The best place to get help with Splunk is the Splunk Community Answers
2. Splunk IS NOT a bussiness intelligence tool. It is an IT operations tool that consumes machine-data in real time. Most Bussiness intelligence tools do not consume and analyze data in real time.
3. Splunk can be used to investigate, alert, build dashboards, and build reports.

Now - Deploying Spunk

but first a note on configuraton files

1. everything Splunk does is governed by configuration files.
2. Configuration files are stored in the /etc, and they have the .conf extension
3. Confugration files are layered. You can have .conf files that have the same name in different directories.
4. NEVER EDIT .conf FILES IN THE DEFAULT DIRECTORY
5. The /etc/<app>/default directory contains preconfigured versions of .conf files.
6. Instead use the Local Directory
7. The /etc/<app>/local directory is where the custom configurations are stored.

Now the Configuration File Structure
.conf files are build on Stanza's and then underneath it attribute values

[Stanza]
Attribute = Value

Splunk offers 2 main deployment models.
Cloud & On Premises

regarldess which deployment you use the data pipeline is the same
IPIS
input - forwarded data, uploaded data, network data, scripts (Splunk is consuming data, but doens't look at it)
parsing - examines the data, addes metadata (Splunk looks at the data)
indexing - data divided into events. Wrties the data to the disk in "buckets" (stores its work in directories, it calls buckets)
searching - user interaction with the data. (self explanetory)

To scale your system, you can spread the functionality accross multiple specialized instances of splunk. No matter how large you scale, you installing only one of two types of splunk instances, a universal fowarder, or a splunk enterprise instalation

This all makes up every splunk deployments

Forwarder =>
(then below the Deployer)
Heavy Forwarder => Search Head => Deployment Server
Master Cluster Node => License Server => Indexer

The 1st deployment, liek personal deployments -

So when you install Splunk on a Computer, that instalation serves as a Search Head and an Indexer that has up to 10 fowarders and appropriate for up to 10 users

The 2nd deployment is enterpise deployment

You have a single search head, two to three indexers, 100-20 fowarders, and appropriate for 100 - 200 users

A 3rd type of Deployment is a Distributed Search

In this config, you build search head clusters governed by a deployer. It distributed files and configuration updates to all members through a configuration bundle.

A search head cluster is a group of search heads that serve as a central resource for searching. So users can log into any member of the search head cluster and take advantage of clustering a bunch of search heads together. (for performance)

Users can run the same searchs and view the same data for any search head in the cluster

One head is designated as the Search Head Captian and schedals jobs between the clusters

With this deployment there are many indexers and thousands of load-balanced forwarders

The 4th type of deployment is has near infinite scalability. In this config,
called a "Large Enterprise Deployment"

It can have Search Head Clusters, or Indexer Clusters, or Both
And it can have thousands of load-balanced fowarders

- Now how does Splunk Store Data

Splunk adds data to Indexes, the indexes match the places on the disk that Splunk calls buckets.

Splunk comes with several indexes built in, and you can create you own as well
Splunk transforms incoming data into events, and stores it in indexes.
An event is a single row of data that has metadata attached to it.
The default index of Splunk is main and it also has an \_internal index that stores internal logs and a few other indexes that come with splunk.

Again, an event is a single row of data
Data is specified by fields (key values pairs) (like objects in JavaScript)
Splunk adds default fields to all events - Timestamp (time), host (ip address), source (the file from where the source originates), sourcetype (the format of the data)

In Splunk, an index contains compressed raw data from an associated index files, and they are spread out into different directory files depending on their age.

There are 6 default buckets, but only 5 important for us to know about

The 6th bucket is the Fish Bucket and its more advanced

1. Hot Bucket (hotpath) is for newly indexed data for it be Read/Write
2. Warm (warmPath) Contains data rolled from Hot Buckets with no active writing (So Read only) And index has many Warm buckets
3. Cold (coldPath) Takes data from warm buckets and moves it to a different locaton. An index has many cold buckets
4. Frozen (frozenPath) data rolled from cold, where data is deleted or archived
5. Thawed (thawedPath) data resored from an Archive

6. Fish Bucket (this is more advanced so we will not talk about it yet)

With Buckets, you can set up retirerment, archive, back up, configure index sizes, partition index data

Splunk Licensing

You license data ingested per day, not data stored

Daily indexing volume is measured from midnight to midnight by the clock on the license master

Internal Log Data, Duplicate Data, and Meta Data do not count against your licesne.

There are 5 splunk licesne types
Standard / Enterprise/ Sales Trial / Dev Test / Free(barebones with ample restrictions)

There is also a few speciality liscense for Industral Lot and Forwarder

If you exceed your licesne limit within a day, you get a warning
get 5 warnings in a 30 day period, you are offically in violation
If you're in violation, it will annoy you with notices, but it won't be enfroced, so you don't lose functionalty. Splunk allows this to accomidate unexpected bursts of data volume

Regarding Splunk Licensing

1. license pools are created from license stacks
2. Pools are sized for specific purposes
3. Managed by the license master
4. Indexers and other Splink enterprise instances are assigned to a pool

Since fowarders have a unique license, the shouldn't be fowarded to a licesne pool

Largest container is a License Group, within those groups you can create a license stack.
A license stack is a stack of licenses added together
License pools then consume all or most of the license stack, and splunk instances are assigned licesnes out of pools.

Splunk also has Apps and Addons for those apps.

The Apps offer better Visulizaton, Analysis, Reports, Dashboards, and user interfaces

While the Add-ons can give data enrichment, tags, data models and data sets.

A App is simply a collection of .conf files
An Add on is a subset of an app, Add-ons specify do not have Graphical user interfaces cause they are add ons to other apps.

Apps are downloaded from Splunkbase.com - 1st and 3rd party apps, premium and free offerings.

Some of the free apps however will require a license.

So with learning Splunk

Getting the data in

So to reiterate, the Splunk Data Pipeline is IPIS
Inputs - Parsing - Indexing - Search

General Input Categories

- File and Directory Inputs
  - This allows you to monitor files and directories
    - Both locally and remotely
    - And you can monitor compressed files
  - Upload
    - upload files to splunk or to a remote Heavy Fowarder
    - used for a one-time analysis
  - MonitorNoHandle
    - Available for Windows Hosts only
    - Monitor files and directories that the system rotates automatically.
- Network Inputs
  - Data from TCP and UDP (Splunk recomends using TCP whenever possiable)
    - syslog
  - Data from SNMP events
- Windows Inputs
  - Windows Event Logs
  - Registry
  - Active Directory
  - WMI (using fowarders is recommended over using WM)
  - Performance Monitoring
    (perfmon)

Splunk can also take in other data sources - Metrics, Scripted Inputs, Modular Inputs, and HTTP Event Collectors

- Ways to Configure Input in Splunk

  - Through an App
    - Many Apps have preconfigured inputs
  - Splunk Web
    - Settings > data inputs
    - Settings > add Data
  - CLI (Command line interface)
    - ./splunk add monitor <path>
  - Through inputs.conf
    - Add a stanza for each input
  - Guided Data Onboarding (GDO)
    - Data input Wizard

- Splunk has two main Fowarder types
  - Universal - Takes Data from a data source and fowards it to a reciever - thats all it does. The data can even be another reciever - However these aren't part of Splunk itself and required a seperate instalation
    The Universal Fowarder also does not search or parse any data, it just captures the data and fowards it on to something else
  - Heavy - An advanced fowarder part of Splunk, Unlike Universal Fowarders, a Heavy Fowarder can not foward data, but also Parse data and route data, among many other things you can't do with a universal fowarder.

Universal Fowarder Configuration Steps 1. Configure recieving on a Splunk Enterprise instance 2. Download and install the UF 3. Start the UF 4. Configure the UF to send data 5. Configure the UF to collect data from the host system

- Side Note - Installing Splunk on my Mac was kind of weird...need to try installing Splunk on Windows

- Summary
  - Splunk Data Pipeline
  - Configureing Inputs
  - Types of forwarders
  - configuring heavy fowarders
  - configuring universal fowarders

Splunk Quiz 2 -

1. The Splunk Enterprise Trail license is valid for? - 60 days
2. To collect and parse data at a source, you need a? - Heavy Fowarder
3. Splunk can be set up in a distributed environment - True
4. Splunk can be installed in the following enviromens - All of these (unix, linux, windows, mac, solaris)
5. Select the best description of a SplunK App - A collection of configuration files that extend the functionality of Splunk
6. A licensing violation is in effect for? - 30 Days
7. Splunk will stop indexing your data during the violation period. - False
8. To manage licensing in the Splunk GUI, navigate to - Settings/Licensing
9. Search is a Splunk App? - True
10. Where should you go to find and download Splunk apps? - splunkbase.com

Splunk Quiz 3 -

1. What is the default forwarding port? - 9997
2. Universal Forwarders do not parse data - True
3. Which port is the default management/deployment port - 8089
4. Which type of forwarder requires a specific type of license? - Heavy
5. On which platform can you use WGET to install a universal forwarder? All of these!
   - Linux, Unix, Windows, Solaris, A/X,
6. Other then the installation wizard, how can you configure a universal forwarder? - By editing the configuration files
7. Universal Fowarders should also be installed on all indexes? - False
8. Some syslog devices do not require Splunk Fowarders. Syslog data is generally recieved on port? - 514
9. Which of the following is not a Splunk default metadata assignment? - Network
10. Splunk can monitor both individual files and entire directories? - True

------------Searching in Splunk Notes---------------- 
What is a Search Processing Language? The technical definition 

SPL encompasses all the search commands and their functions, arguments, and clauses. its syntax was originally based on the Unix pipeline and SQL. The scope of SPL includes data searching, filtering, modification, manipulation, insertion, and deletetion. 

But what does all that mean? What does that mean to search in Splunk?
- its the primary way users interact with data in Splunk
  - Query
  - Calculate
  - Transform
  - Organize
  - Visualize
  - Manipulate 

We do this thru the Search and Reporting App
  - Its the Default App
    - Comes built-in to Splunk
  - Primary Way to search and analyze data in Splunk
    - Index Data
    - Build reports and visualizations
    - configure alerts
    - create dashboards

--- Time ---

Splunk searchs in the data for timestamps
- Timestamps are converted to UNIX time and stored in the _time field
- Splunk assumes that any data indexed is in the time zone of the splunk instance 

The _time Field is very important (underscore time field)

- A default, and essential field 
- Values stored in the _time field are stored in UNIX time
- In Splunk Web, the _time field appears in human readable format
- Use search commands to manipulate the time format

It looks for timestamps in event data - if there is no timestamp, Splunk looks for the time information associated with the source name or file name.
If it can't find anything there, it will look at the file modification time, and if all else fails, as a last resort it will use the current system time 

Bottom line, Splunk will not let us have Data without Time 

One way Splunk uses time is for the Time Range Picker
- Splunk uses time stamps for the time range picker
- The defaults for the time range picker are Real-Time, Relative, and Other
- We can also do some more granular things with time selection 

Specify Absolute Time Ranges using SPL
- specify time ranges directly in your search
- for absolute ranges
- for relative ranges:
  - + or - to indicate the offset
  - Number
  - Time unit (years y, quarters q, months mon, weeks w, days d, hours h, minutes m, seconds s)

Time Variables 
- format time using time variables
- useful when evaluating time and specifying time in SPL

Variable - Description 
%c       - Date and time in the format of the server 
%H       - Hour (24-hour clock)
%I       - Hour (12-hour clock)
%M       - Minutes (00 - 59)
%p       - AM or PM 
%S       - Seconds (00 - 59) 
(there are alot more, too many to rewrite here)

Converting Time using strftime
- We can convert time in the format we want during search time using and eval expression, strftime, and time variables on the _time field 

Kind of really complicated but 

_time          ---->   New_Time
160617990              1:05, PM 

Short Quiz

So its currently Monday January 1st, 1900 at 5:00pm

String                        -     Answer
%A, %B, %d, %Y - %I:%M %p     -     Monday, January 01, 1900 - 5:00pm 

Basic Searching 

Broad Search Terms - metadata

When begining a search, we we know these, we should start with these 

- Index
  - index = mian, index = default

- Host
  - host = server.com, host = 192.168.1.1

- Source, sourcetype
  - source = /var/lib, sourcetype = csv

Basic Search Commands
- Chart / timechart
  - returns results in tabular output for charting
- rename 
  - Renames a specific field
- sort
  - Sort results by specified fields
- states
  - Statistics
- eval
  - Calculates an expression
- dedup 
  - removes duplicates
- table
  - builds a table with specific fields 

Constructing a Basic Search 
Search Terms | Commands

----- Fields ------
- Searchable key-value pairs
- Key = Value 
  user = user1
  ip_addr = 192.168.1.1
  message = error
  host = websvr.com

Most of the data we feed Splunk won't have key-value pairs, but that doesn't stop splunk 
- Splunk automaically discovers fields 
  - first it discovers default fields - Host, source, sourcetype, time, etc.
  - then it looks for obvious key-value pairs in the first 100 events of the data 
  - then it builds fields using custom field extractions built by the user or by an app. These field extractions are stored in conf. files

  Splunk has three levels of field / search discovery 
  1. Fast
  2. Smart
  3. Verbose 

  Each has their own specific use case and each can effect the performance of splunk differently. 

  Smart is the default mode 

  If your data has clear key-value pairs, Splunk doesn't have to work so hard so it would switch automatically from Smart to Fast. Fast mode bascially disables field discovery 

  Verbose is used when you don't know much about the the data and you need to harness the full power of Splunk, tradeoff is the speed, it will take much longer 

  Field Extractions - Custome field extractions can be built using the Splink field extractor 

  Uses regular expressions (regex) to extract fields based on patterns 


// So in the demo section, lets solve a bussiness problem with a basic search 

The Bussiness asks What the backup duration was for each domain in the last 30 days 

1. we start a new search, using the homework data set of splunkmain and set the time for the last 30 days. I got 120 events.

Under interesting fields, Backupduration is first. 

down on that list is domian, in which we have 5 domains 

we will search both of those fields with wildcards

host=splunkmain backupduration=* domain=*

but to make a table out of it

host=splunkmain backupduration=* domain=* | table _time backupduration domain 

after you se4arch that, click visualization and by default it will build a column chart 

however since domain isn't a numerical value, we can remove it and search again

host=splunkmain backupduration=* domain=* | table _time backupduration

No we have a trend with backup duration 

now we save panel to new dashboard

now we search for the domain again since thats what the bussiness asked for

it would also be useful to on the dashboard to display the average time that backups are taking 

for that we use a very popular command called stats with the keyword avg, and in parentheses, the field we want the average of 

host=splunkmain backupduration=* domain=* | stats avg(backupduration)

this will give us the average we want

with my dataset, I get an average backupduration of 8.9, which rounds to 9

select visualiztion and select single value ( do not underestimate the power of a single value visualization)

then we go to format and change the caption to Average Backup Duration last 30 days, and then change the number format to give us two decimal places
and then we set the unit as hours

and what the hell, lets use colors, switch the color mode on the bottom to color the background 

But lets say we want to change the color to yellow if the backup duration is over 9 hours. So under 9 hours we're good, over 9 hours we're bad and in warning status, and 15 and above we are in critical

Now lets make a panel that shows the maximum backup duration by domain 


host=splunkmain backupduration=* domain=* | stats max(backupduration) by domain

this will give us the domains with the backups taking the longest 

format the data change the caption to Domain with Longest Backup Duration 

Now we can view the dashboard and make it prettier 

TIME- 

to be honest, this new search query confuses me

Its basicly formating the data for the search 

host=splunkmain | eval new_time=strftime(_time, "%m-%d-%y %I:%M%p") | table _time new_time

%m-%d-%y is for month, day, and year

Field Extractions - not even sure how to take notes for this part, just rewatch the video a couple of itmes. Section 5 -30 (5th part of the demo)

Intermediate Searching >
Recall the search pipline - we start with alot of data, and we widdle it down to the data we want in the format we want
Broad Search => Keywords/Booleans/Fields => Commangs => Table/Viz

The most popular transforming commands
- Top
  - | top <field>
  - Returns the most common values of a given field
  - Defaults to 10 fields 
  - Can be combined with limit=<number>
  - Automatically builds a table with count and percent columns
  - Can be used with multiple fields 
    - "return the top value for a field organized by another field"
- Rare
  - | rare <field> (same syntax as top)
  - Opposite of top
  - Returns the least common values of a field 
  - Options are identical to top
- Stats
  - more complicated syntax
  - | stats <function(field)> By <field(s)>
  - Some common functions
    - count, avg, max, mean, median, sum, stdev, values, list 

some examples of stats

| stats avg(kps) BY host - 

AVG(kps)          host
654.78            host1.domain.com

| stats count(failed_logins) BY user

failed_logins      user
42                  jwebber

Like to pull up a search of the homework data by state

host=splunkmain state=* usr=* | stats count(usr) BY state

this gives us a statisitcal table listing every state with the number of users 

slight modification adding AS cuser

host=splunkmain state=* usr=* | stats count(usr) AS cuser BY state

and a further modifcation that sorts it and puts the highest number at the top

host=splunkmain state=* usr=* | stats count(usr) AS cuser BY state | sort -cuser

___________________________________________________

another example, using the interesting fields of level

- host=splunkmain state=* level=critical | top state by level

searching that gives us the level, state, count, and percent

but then we try rare, which gives is the opposite of top

- host=splunkmain state=* level=critical | rare state by level


Quiz - 
Question 1 - 

Which of the following needs to be placed in quotes

Keywors, Phrases, Commands, Transformations 

the answer is Phrases 

Question 2 - 

Which search mode does not discover fields? 

Fast, Verbose, Smart, No_Fields

the answer is Fast

Question 3 - 

The time 11:33 PM can be expressed in the following Splunk variables: 

the answer is %I:%S %p

Question 4 - 

The data Monday, February 23, 1085 can be expressed in the following Splunk variable

the answer is %A, %B %e, %Y

Question 5 - 

Which of the following is not an option for extracting fields
ddex, regex, delimiters

the answer is ddex

question 6 - 

Indices are "buckets" where Splunk data is stored on disk

True

Question 7 - 

Splunk detects fields as 

key=value pairs

Question 8 - 

The basic search pipeline goes from general to specific 

Question 9 - 

The Search app comes built into Splunk enterprise

true

Question 10 - 

SPL stands for 

Search Processing Language 

-- Visualizing Your Data ---

- The basics of visualization 
- modeling data using data models
- reporting and alerting 
- The Pivot Tool

Widly liked Quote on the subject

"Success in Data Visualization does not start with data visualization."

Why Visualize Data?

- Data alone is not very interesting to look at
- non-technical people might not be interested in tabular data
- visualizing data is more human centric
  - Humans are pattern recognizing machines
  - From an early age we are taught to visualize data (blocks with letters)
  - Visualizations are art and therefore have an emotional impact on us. 

Some Vocabulary 

Dimensions - Something you want to measure 

Data Visualization Contesxt
- Understand your audience 
  - Executives usually have different needs than individual contributors 
  - Publically available dashboards might display different data than       internal-only reports and dashboards
- Understand Your Own Goals
  - What is the message for which you are using this visualization
  - like the famous "authors purpose" consider the PIE: Do you want to persuade, inform, or entertain? 

The Types of Visualizations in Splunk are the most typical

Piecharts, Linecharts, Area Charts, Column Charts, tables, guages, and never underestimate the power of a single value.

Data Models - 
  - Make machine data easier to understand\
  - simplify complex data through abstraction 
  - group specific types of data

Data models are used all the time. math equations are essentially data models 

Data Model Structure
- Stacks of datasets
- Datasets are stacks of knowledge objects
- Knowledge objects include saved searches, field extractions, tags, and more 

- Events - most commonly used 
  - Event Constraints - Must include an index constraint (index=)
  - Searches - Splunk saved searches that include transforming commands, etc. 
  - Search Constraints - constrained to the full search string (must include an index constraint (index=))
  - Transactions - combine multiple events from one or many sources into a single event 
  - Transaction Constraints - must be legally formed transaction search 

- Fields 
  - Can be added to roots and children 
  - Children inherit all fields from their parent
    - Auto Extracted - Splunk automatically discovered fields
    - Eval - A field generated as a result of an eval expression 
    - Lookup - Fields that are the result of a lookup
    - Regular expression - fields extracted by regex  

Reporting and Alerting 

- Reports and alerts are knowledge objects in Splunk 
- To create reports and alerts, you need a splunk exterprise license 
  - The Free version disables reports and alerts

What is a Report? 
- Saved Searches that can run on a schedule and perform an action 
  - send an e-mail to report consumers
  - embed on a web page
  - update a dashboard panel
  - run a script 
  - Scheduled reports can run on the time your define, hourly, weekly, daily, monthly, etc

- You can stagger the report runnign window 
  - useful if you have a lot of reports running at the same time
- You can also set an alert
  - Alert Actions can include
    - send an email
    - trigger a script
    - use a webhook
    - list in triggered alerts
    - USe and app (Like Pagerduty ot Slack)


- The Pivot Tool - Why use it?
  - Create dashboards, reports, and alerts without using SPL
  - Provides a drag and drop interface to Splunk users
  - Pivot functionality is built on data models 

Basic Pivot functions include; filter, split by row, split by column, column value  

All pivots use a time range, and the more narrowly you define the time range, the better

- Deployment and Forwarder Management

- Deployment Server

- A server class defines a group fo Splunk deployment apps and add them to its member criteria
- each client of a server class reconciles its apps with the server
  - if it is missing any, it pulls them from the deployment server
- Deployment apps are located in /etc/deploymentapps
- to manage deployment servers, go to settings, Distributed environment, forwarder management 

Users, Roles, and Authentication 

Splunk Users
Settings > Access Controls

- can be defined locally
- can be defined in a directory like LDAP or AD (active directory)

Five Built in Roles
  1. admin
  2. power
  3. user
  4. can_delete
  5. splunk-system-role

Splunk administrators can also create custom roles
- some apps come with custom roles
  - winfra-admin, vmware_admin, etc. 

Authenticated Options
- local
- LDAP
- SAML
- Scripted SSO

- Splunk recommneds using LDAP to manage authentication
- Splunk works with Open LDAP an Active Directory

